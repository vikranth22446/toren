#!/usr/bin/env python3

import json
import os
import re
import subprocess
from typing import Any, Dict, Optional


class AICliInterface:
    # Timeout constants (in seconds)
    CLI_COMMAND_TIMEOUT = 30  # For Claude CLI API calls

    def __init__(self, cli_type: str = "claude"):
        self.cli_type = cli_type
        self.cli_configs = {
            "claude": {
                "command": "claude",
                "api_key_env": "ANTHROPIC_API_KEY",
                "model": "claude-3-5-sonnet",
                "print_flag": "--print",
            },
            "gemini": {
                "command": "gemini",
                "api_key_env": "GEMINI_API_KEY",
                "model": os.environ.get("GEMINI_MODEL", "gemini-2.5-flash"),
                "print_flag": "-p",
            },
            "codex": {
                "command": "codex",
                "api_key_env": "OPENAI_API_KEY",
                "model": "gpt-5",
                "print_flag": "exec",
            },
        }
        self.config = self.cli_configs.get(cli_type, self.cli_configs["claude"])

    def get_api_key(self) -> Optional[str]:
        api_key_env = self.config["api_key_env"]
        if api_key_env in os.environ and os.environ[api_key_env].strip():
            return os.environ[api_key_env]
        return None

    def get_language_config(self, language: str) -> Dict[str, str]:
        configs = {
            "python": {
                "security_tools": "bandit safety pip-audit",
                "test_command": "python -m pytest",
                "lint_command": "python -m flake8",
                "build_command": "python -m pip install -e .",
                "security_scan": "bandit -r . && safety check && pip-audit",
            },
            "rust": {
                "security_tools": "cargo-audit cargo-deny",
                "test_command": "cargo test",
                "lint_command": "cargo clippy -- -D warnings",
                "build_command": "cargo build",
                "security_scan": "cargo audit && cargo clippy && cargo deny check",
            },
        }
        return configs.get(language, configs["python"])

    def estimate_task_cost(
        self, task_content: str, language: str = "python"
    ) -> Optional[Dict[str, Any]]:
        try:
            print("ğŸ’° Asking Claude to estimate task cost...")

            estimation_prompt = f"""Analyze this task specification and estimate the \
Claude API cost to complete it:

TASK SPECIFICATION:
{task_content}

TARGET LANGUAGE: {language}

Please analyze and provide:
1. Task complexity (simple/medium/complex)
2. Estimated tokens needed (input + output)
3. Estimated cost in USD
4. Key factors affecting cost
5. Suggestions to reduce cost if applicable

Consider:
- Code analysis and understanding needed
- Amount of code to be written/modified
- Testing requirements
- Documentation needs
- Number of files likely to be read/modified

Respond with a JSON object in this format:
{{
    "complexity": "simple|medium|complex",
    "estimated_input_tokens": 1000,
    "estimated_output_tokens": 300,
    "estimated_total_cost": 0.0234,
    "cost_factors": ["factor1", "factor2"],
    "cost_reduction_tips": ["tip1", "tip2"],
    "confidence": "high|medium|low"
}}"""

            result = subprocess.run(
                [self.config["command"], self.config["print_flag"], estimation_prompt],
                capture_output=True,
                text=True,
                timeout=self.CLI_COMMAND_TIMEOUT,
            )

            if result.returncode == 0:
                output = result.stdout.strip()

                json_match = re.search(r"\{[^}]+\}", output, re.DOTALL)
                if json_match:
                    json_str = json_match.group(0)
                    estimate_data = json.loads(json_str)

                    estimate_data["language"] = language
                    estimate_data["model"] = self.config["model"]
                    estimate_data["raw_response"] = output

                    return estimate_data
                else:
                    return {
                        "complexity": "unknown",
                        "estimated_total_cost": 0.02,
                        "raw_response": output,
                        "language": language,
                        "model": self.config["model"],
                        "confidence": "low",
                    }
            else:
                print(f"âš ï¸  Warning: Cost estimation failed: {result.stderr}")
                return None

        except subprocess.TimeoutExpired:
            print("âš ï¸  Warning: Cost estimation timed out")
            return None
        except Exception as e:
            print(f"âš ï¸  Warning: Could not estimate cost: {e}")
            return None

    def print_cost_estimate(
        self, estimate: Dict[str, Any], task_type: str = "task"
    ) -> None:
        if not estimate:
            print("âŒ Cost estimation unavailable")
            return

        print(f"\nğŸ’° **{self.cli_type.title()}'s Cost Estimate for {task_type}**")
        print("-" * 50)

        if "complexity" in estimate:
            complexity_emoji = {"simple": "ğŸŸ¢", "medium": "ğŸŸ¡", "complex": "ğŸ”´"}.get(
                estimate["complexity"], "âšª"
            )
            print(f"{complexity_emoji} Complexity: {estimate['complexity'].title()}")

        if "estimated_input_tokens" in estimate:
            print(f"ğŸ”¤ Est. Input Tokens: {estimate['estimated_input_tokens']:,}")
        if "estimated_output_tokens" in estimate:
            print(f"ğŸ“ Est. Output Tokens: {estimate['estimated_output_tokens']:,}")

        if "estimated_total_cost" in estimate:
            print(f"ğŸ’° **Estimated Cost: ${estimate['estimated_total_cost']:.4f}**")

        if "confidence" in estimate:
            conf_emoji = {"high": "ğŸ¯", "medium": "ğŸ“Š", "low": "ğŸ¤”"}.get(
                estimate["confidence"], "â“"
            )
            print(f"{conf_emoji} Confidence: {estimate['confidence'].title()}")

        print(f"ğŸ¤– Model: {estimate.get('model', self.config['model'])}")
        print(f"ğŸ”§ Language: {estimate.get('language', 'unknown')}")

        if "cost_factors" in estimate and estimate["cost_factors"]:
            print("\nğŸ“‹ **Cost Factors:**")
            for factor in estimate["cost_factors"]:
                print(f"  â€¢ {factor}")

        if "cost_reduction_tips" in estimate and estimate["cost_reduction_tips"]:
            print("\nğŸ’¡ **Cost Reduction Tips:**")
            for tip in estimate["cost_reduction_tips"]:
                print(f"  â€¢ {tip}")

        if "raw_response" in estimate:
            print("\nğŸ” **Raw Response Preview:**")
            print(
                estimate["raw_response"][:300] + "..."
                if len(estimate["raw_response"]) > 300
                else estimate["raw_response"]
            )

        print()

    def check_short_description_quality(self, short_description: str) -> Dict[str, Any]:
        """
        Check if a short description is clear and actionable enough for task execution.
        Returns quality assessment with clarity score and recommendations.
        """
        try:
            print("ğŸ” Checking description quality...")

            quality_prompt = f"""Analyze this short task description for clarity and actionability:

TASK DESCRIPTION:
{short_description}

Please assess if this description is clear enough for an AI agent to \
understand and execute the task. Consider:

1. Is the goal/objective clear?
2. Are the requirements specific enough?
3. Is the scope well-defined?
4. Are there enough actionable details?
5. Are there any ambiguities that could lead to wrong implementation?

Respond with a JSON object in this format:
{{
    "is_clear": true/false,
    "clarity_score": 1-10,
    "issues": ["list of specific issues if any"],
    "recommendations": ["suggestions to improve clarity"],
    "assessment": "brief overall assessment"
}}

Be strict - only mark as clear (true) if the description provides \
enough detail for confident implementation."""

            result = subprocess.run(
                [self.config["command"], self.config["print_flag"], quality_prompt],
                capture_output=True,
                text=True,
                timeout=self.CLI_COMMAND_TIMEOUT,
            )

            if result.returncode == 0:
                output = result.stdout.strip()

                json_match = re.search(r"\{[^}]+\}", output, re.DOTALL)
                if json_match:
                    json_str = json_match.group(0)
                    quality_data = json.loads(json_str)
                    quality_data["raw_response"] = output
                    return quality_data
                else:
                    # Fallback if JSON parsing fails
                    return {
                        "is_clear": False,
                        "clarity_score": 3,
                        "issues": ["Could not parse quality assessment"],
                        "recommendations": ["Try providing more specific details"],
                        "assessment": "Assessment failed - assuming unclear",
                        "raw_response": output,
                    }
            else:
                print(f"âš ï¸  Warning: Quality check failed: {result.stderr}")
                return {
                    "is_clear": False,
                    "clarity_score": 3,
                    "issues": ["Quality check service unavailable"],
                    "recommendations": ["Provide more detailed specification"],
                    "assessment": "Could not assess quality",
                }

        except subprocess.TimeoutExpired:
            print("âš ï¸  Warning: Quality check timed out")
            return {
                "is_clear": False,
                "clarity_score": 3,
                "issues": ["Quality check timed out"],
                "recommendations": ["Try with a more detailed specification"],
                "assessment": "Assessment timeout",
            }
        except Exception as e:
            print(f"âš ï¸  Warning: Quality check error: {e}")
            return {
                "is_clear": False,
                "clarity_score": 3,
                "issues": [f"Error during assessment: {str(e)}"],
                "recommendations": ["Provide more detailed specification"],
                "assessment": "Assessment error",
            }

    def print_quality_assessment(self, quality: Dict[str, Any]) -> None:
        """Print formatted quality assessment results"""
        print("\nğŸ” **Task Description Quality Assessment**")
        print("-" * 50)

        if "clarity_score" in quality:
            score = quality["clarity_score"]
            if score >= 8:
                score_emoji = "ğŸŸ¢"
                score_desc = "Excellent"
            elif score >= 6:
                score_emoji = "ğŸŸ¡"
                score_desc = "Good"
            elif score >= 4:
                score_emoji = "ğŸŸ "
                score_desc = "Fair"
            else:
                score_emoji = "ğŸ”´"
                score_desc = "Poor"

            print(f"{score_emoji} Clarity Score: {score}/10 ({score_desc})")

        if "is_clear" in quality:
            clear_emoji = "âœ…" if quality["is_clear"] else "âŒ"
            clear_status = "Clear" if quality["is_clear"] else "Unclear"
            print(f"{clear_emoji} Overall Assessment: {clear_status}")

        if "assessment" in quality:
            print(f"ğŸ“ Summary: {quality['assessment']}")

        if "issues" in quality and quality["issues"]:
            print("\nâš ï¸  **Issues Found:**")
            for issue in quality["issues"]:
                print(f"  â€¢ {issue}")

        if "recommendations" in quality and quality["recommendations"]:
            print("\nğŸ’¡ **Recommendations:**")
            for rec in quality["recommendations"]:
                print(f"  â€¢ {rec}")

        print()
